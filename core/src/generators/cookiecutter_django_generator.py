"""
Django project generator based on Cookiecutter.
Migration from the previous system to use official cookiecutter-django templates.
"""
import os
import sys
import subprocess
import tempfile
import shutil
import json
import logging
import stat
import re
import hashlib
import time
from pathlib import Path
from core.src.generators.base_generator import BaseProjectGenerator
from core.src.utils.project_utils import FileSystemUtils
from core.src.utils.generator_utils import (
    DependencyManager, TemplateManager, FileOperations, 
    DockerUtils, SecurityTools, PipelineGenerator, _run_command_safe, _check_command_availability, _get_command_executable
)
from core.src.generators.templates.django_pipeline_templates import (
    DJANGO_PIPELINE_DEV_TEMPLATE,
    DJANGO_PIPELINE_TEST_TEMPLATE,
    DJANGO_PIPELINE_PROD_TEMPLATE
)


class CookiecutterDjangoGenerator(BaseProjectGenerator):
    """Django generator using cookiecutter with automatic improvements."""
      # Django-specific reserved names (common ones are in ProjectValidator.COMMON_RESERVED_NAMES)
    DJANGO_RESERVED_NAMES = {
        'wsgi', 'asgi', 'celery', 'channels', 'drf', 'rest_framework',
        'middleware', 'context_processors', 'validators', 'serializers'
    }
    def __init__(self, project_name: str, output_dir: str, template_url: str = None, interactive: bool = False):
        """
        Initializes the cookiecutter generator.
        
        Args:
            project_name: Name of the project to generate
            output_dir: Directory where the project will be created
            template_url: URL of the cookiecutter template (optional)
            interactive: Whether to use cookiecutter's interactive mode
        """        
        # Use centralized validation with Django-specific reserved words
        super().__init__(project_name, output_dir, self.DJANGO_RESERVED_NAMES)
        self.template_url = template_url or 'https://github.com/cookiecutter/cookiecutter-django.git'
        self.interactive = interactive
        self.custom_config = {}
        
        # Setup local template cache
        self.local_templates_dir = os.path.join(output_dir, '.cookiecutter_templates')
        self.template_cache_path = None
        
        self._ensure_dependencies()
      
    def _get_cache_dir(self) -> str:
        """
        Get or create a cache directory for templates to avoid repeated cloning.
        """
        return FileSystemUtils.get_cache_directory("localforge")
    
    def get_project_type(self) -> str:
        """Returns the project type."""
        return "django-cookiecutter"
    
    def get_directory_structure(self) -> list:
        """Not applicable for cookiecutter - structure is defined by the template."""
        return []
    
    def get_project_files(self) -> dict:
        """Not applicable for cookiecutter - files are generated by the template."""
        return {}
    def create_project(self) -> bool:
        """
        Creates the project using cookiecutter and applies automatic improvements.
        Uses the base class validation and cleanup, but with cookiecutter-specific logic.
        
        Returns:
            bool: True if the project was successfully created, False otherwise
        """
        try:
            # Check dependencies first
            self._ensure_dependencies()
            
            # Let base class handle validation and directory creation
            logging.info(f"üöÄ Generating Django project '{self.project_name}' with cookiecutter...")
              # 1. Generate base project with cookiecutter to a temp location
            temp_project_path = self._generate_with_cookiecutter()
            
            # 2. The project has already been moved to the correct location by _generate_with_cookiecutter
            # No need for additional moving since _move_generated_project is called internally
            
            # 3. Apply automatic improvements to the final location
            logging.info("üîß Applying automatic improvements...")
            final_path = str(self.project_path)
            self._add_security_tools(final_path)
            self._optimize_dockerfile(final_path)
            self._update_docker_compose(final_path)
            self._create_precommit_config(final_path)
            self._setup_development_settings(final_path)
            self._create_pipeline(final_path)
            
            logging.info(f"‚úÖ Django project '{self.project_name}' successfully created with cookiecutter")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Error creating Django project: {e}")
            return False

    def _ensure_dependencies(self):
        """Ensures that the necessary dependencies are installed using centralized dependency manager."""
        # Use centralized dependency management
        cookiecutter_available = DependencyManager.ensure_cookiecutter()
        pyyaml_available = DependencyManager.ensure_pyyaml()
        git_available = DependencyManager.ensure_git()
        
        if not cookiecutter_available:
            raise Exception("Cookiecutter is required but could not be installed")
        
        if not git_available:
            raise Exception(
                "Git is required to clone cookiecutter templates but is not available. "
                "Please install Git and ensure it's in your PATH."
            )
        
        if not pyyaml_available:
            logging.warning("PyYAML not available - some features may be limited")
        
        # Check for yq (optional, with fallback)
        self._has_yq = DependencyManager.check_system_command('yq')
        if not self._has_yq:
            logging.info("‚ÑπÔ∏è  yq not available, using Python fallback")
    
    def _apply_custom_config(self, custom_config: dict):
        """
        Applies custom configuration for cookiecutter.
        
        Args:
            custom_config: Dictionary with custom form values
        """
        self.custom_config = custom_config
    
    def _generate_with_cookiecutter(self) -> str:
        """
        Generates the project using cookiecutter with local template caching.
        
        Returns:
            str: Path of the generated project
        """
        # Clean up old templates before starting
        self._cleanup_old_templates()
        
        # Prepare template (cached or clone)
        template_dir = self._prepare_template()
        
        try:
            # Temporary directory for output
            temp_output = tempfile.mkdtemp(prefix='cc_django_')
            
            if self.interactive:
                # Interactive mode
                self._run_interactive_cookiecutter(template_dir, temp_output)
            else:
                # Automated mode
                self._run_automated_cookiecutter(template_dir, temp_output)
            
            # Move project to final destination
            generated_path = self._move_generated_project(temp_output)
            
            return generated_path
            
        finally:
            # Clean up temporary files (but keep template cache)
            if 'temp_output' in locals() and os.path.exists(temp_output):
                FileOperations.safe_rmtree(temp_output)
    
    def _prepare_template(self) -> str:
        """
        Prepares the template for use with local caching system.
        
        Returns:
            str: Path to the prepared template (either cached or newly cloned)
        """
        try:
            # If it's a local path, use it directly
            if not self.template_url.startswith(('http://', 'https://', 'git@')) and not self.template_url.endswith('.git'):
                if os.path.exists(self.template_url):
                    logging.info(f"üìÇ Using local template: {self.template_url}")
                    return self.template_url
                else:
                    raise FileNotFoundError(f"Local template not found: {self.template_url}")
            
            # For remote templates, use local caching
            return self._get_or_clone_template()
            
        except Exception as e:
            raise Exception(f"Error preparing template: {e}")
    
    def _get_or_clone_template(self) -> str:
        """
        Gets the template from local cache or clones it if not present.
        
        Returns:
            str: Path to the cached template
        """
        # Create cache directory if it doesn't exist
        os.makedirs(self.local_templates_dir, exist_ok=True)
        
        # Generate cache key from template URL
        template_hash = hashlib.md5(self.template_url.encode()).hexdigest()[:10]
        template_name = self.template_url.split('/')[-1].replace('.git', '')
        cache_dir_name = f"{template_name}_{template_hash}"
        
        self.template_cache_path = os.path.join(self.local_templates_dir, cache_dir_name)
        
        # Check if template is already cached and valid
        if self._is_template_cache_valid():
            logging.info(f"üìã Using cached template: {cache_dir_name}")
            return self.template_cache_path
        
        # Clone or update the template
        return self._clone_template()
    
    def _is_template_cache_valid(self) -> bool:
        """
        Checks if the cached template is valid and up-to-date.
        
        Returns:
            bool: True if cache is valid, False otherwise
        """
        if not os.path.exists(self.template_cache_path):
            return False
        
        # Check if cookiecutter.json exists (basic validity check)
        cookiecutter_json = os.path.join(self.template_cache_path, 'cookiecutter.json')
        if not os.path.exists(cookiecutter_json):
            logging.warning(f"‚ö†Ô∏è Invalid cached template (missing cookiecutter.json): {self.template_cache_path}")
            return False
        
        # Check cache age (optional: refresh if older than 24 hours)
        try:
            cache_info_file = os.path.join(self.template_cache_path, '.cache_info')
            if os.path.exists(cache_info_file):
                with open(cache_info_file, 'r') as f:
                    cache_data = json.load(f)
                
                # Check if cache is older than 24 hours
                cache_time = cache_data.get('timestamp', 0)
                current_time = time.time()
                cache_age_hours = (current_time - cache_time) / 3600
                
                if cache_age_hours > 24:
                    logging.info(f"üîÑ Template cache is {cache_age_hours:.1f}h old, will refresh")
                    return False
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Error reading cache info: {e}")
        
        return True
    
    def _clone_template(self) -> str:
        """
        Clones the template to the local cache directory.
        
        Returns:
            str: Path to the cloned template
        """
        # Remove existing cache if present
        if os.path.exists(self.template_cache_path):
            logging.info(f"üóëÔ∏è Removing old cached template: {self.template_cache_path}")
            FileOperations.safe_rmtree(self.template_cache_path)
        
        # Ensure git is available
        if not _check_command_availability('git'):
            raise Exception(
                "Git is required to clone cookiecutter templates but is not available. "
                "Please install Git and ensure it's in your PATH."
            )
        
        logging.info(f"üì• Cloning template to local cache: {self.template_url}")
        
        try:
            # Clone the template
            result = _run_command_safe([
                'git', 'clone', '--depth', '1', self.template_url, self.template_cache_path
            ], stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, timeout=120)
            
            if result.returncode != 0:
                error_msg = result.stderr.decode() if result.stderr else f"Git clone failed with exit code {result.returncode}"
                raise subprocess.CalledProcessError(result.returncode, 'git clone', stderr=error_msg)
            
            # Create cache info file
            self._create_cache_info()
            
            logging.info(f"‚úÖ Template cached successfully: {os.path.basename(self.template_cache_path)}")
            return self.template_cache_path
            
        except subprocess.TimeoutExpired:
            error_msg = f"Git clone timed out for {self.template_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        except subprocess.CalledProcessError as e:
            error_msg = f"Failed to clone template: {e.stderr.decode() if e.stderr else str(e)}"
            logging.error(error_msg)
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"Unexpected error cloning template: {e}"
            logging.error(error_msg)
            raise Exception(error_msg)
    
    def _create_cache_info(self):
        """Creates a cache info file with metadata about the cached template."""
        try:
            cache_info = {
                'template_url': self.template_url,
                'timestamp': time.time(),
                'cached_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'cache_version': '1.0'
            }
            
            cache_info_file = os.path.join(self.template_cache_path, '.cache_info')
            with open(cache_info_file, 'w') as f:
                json.dump(cache_info, f, indent=2)
                
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Could not create cache info file: {e}")
    
    def _cleanup_old_templates(self):
        """Cleans up old template caches to save disk space."""
        try:
            if not os.path.exists(self.local_templates_dir):
                return
            
            current_time = time.time()
            for item in os.listdir(self.local_templates_dir):
                item_path = os.path.join(self.local_templates_dir, item)
                if os.path.isdir(item_path):
                    # Check age of directory
                    dir_age = current_time - os.path.getctime(item_path)
                    # Remove caches older than 7 days
                    if dir_age > (7 * 24 * 3600):
                        logging.info(f"üóëÔ∏è Removing old template cache: {item}")
                        FileOperations.safe_rmtree(item_path)
                        
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Error during template cleanup: {e}")
    
    def _run_interactive_cookiecutter(self, template_dir: str, output_dir: str):
        """Runs cookiecutter in interactive mode."""
        cmd = [
            'cookiecutter', template_dir,
            '--output-dir', output_dir,
            '--overwrite-if-exists'
        ]
        
        print("\n" + "="*50)
        print("üéØ Django Project Configuration (interactive mode)")
        print("="*50)
        
        # Use safe command execution
        result = _run_command_safe(cmd, timeout=300)
        if result.returncode != 0:
            raise subprocess.CalledProcessError(result.returncode, ' '.join(cmd))
    
    def _run_automated_cookiecutter(self, template_dir: str, output_dir: str):
        """Runs cookiecutter in automated mode with predefined configuration."""
        # Read configuration from cookiecutter.json
        config_path = os.path.join(template_dir, 'cookiecutter.json')
        if not os.path.exists(config_path):
            raise Exception("cookiecutter.json not found in the template")
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Use the already validated project name as slug
        # The project_name has already been sanitized in __init__
        project_slug = self.project_name
        
        # Specific defaults for cookiecutter-django
        defaults = {
            'project_name': self.project_name,
            'project_slug': project_slug,
            'description': f'Django Application: {self.project_name}',
            'author_name': 'Developer',
            'domain_name': 'example.com',
            'email': 'dev@example.com',
            'version': '0.1.0',
            'open_source_license': '1',  # MIT
            'username_type': '1',  # username
            'timezone': 'UTC',
            'windows': 'n',
            'editor': '3',  # VS Code
            'use_docker': 'y',
            'postgresql_version': '2',  # 16
            'cloud_provider': '4',  # None
            'mail_service': '9',  # Other SMTP
            'use_async': 'n',
            'use_drf': 'n',
            'frontend_pipeline': '1',  # None
            'use_celery': 'n',
            'use_mailpit': 'n',
            'use_sentry': 'n',
            'use_whitenoise': 'y',
            'use_heroku': 'n',
            'ci_tool': '1',  # None
            'keep_local_envs_in_vcs': 'y',
            'debug': 'n'
        }
        
        # Apply custom configuration if exists
        defaults.update(self.custom_config)
        
        # Prepare automatic responses
        input_values = []
        for key in config.keys():
            if key in defaults:
                input_values.append(str(defaults[key]))
            else:
                # For unknown keys, use the first default value
                if isinstance(config[key], list) and len(config[key]) > 0:
                    input_values.append(str(config[key][0]))
                elif isinstance(config[key], str):
                    input_values.append(config[key])
                else:
                    input_values.append('')
        
        input_text = '\n'.join(input_values) + '\n'
        
        # Run cookiecutter with automatic responses
        cmd = ['cookiecutter', template_dir, '--output-dir', output_dir, '--overwrite-if-exists']
        
        try:
            # Use safe command execution with input
            process = subprocess.Popen(
                [_get_command_executable(cmd[0])] + cmd[1:], 
                stdin=subprocess.PIPE, 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE, 
                text=True
            )
            stdout, stderr = process.communicate(input=input_text, timeout=300)
            
            if process.returncode != 0:
                raise Exception(f"Error running cookiecutter: {stderr}")
                
        except subprocess.TimeoutExpired:
            process.kill()
            raise Exception("Cookiecutter execution timed out")
    
    def _move_generated_project(self, temp_output: str) -> str:
        """Moves the generated project to the final destination."""
        entries = [d for d in os.listdir(temp_output) if not d.startswith('.')]
        if not entries:
            raise Exception("No project was generated")
        
        # Find the directory of the generated project
        src = os.path.join(temp_output, entries[0]) if len(entries) == 1 else temp_output
        dest = self.project_path
        
        if os.path.exists(dest):
            raise Exception(f"The destination already exists: {dest}")
        
        shutil.move(src, dest)
        return str(dest)
    
    def _detect_docker_usage(self, proj_path: str) -> tuple:
        """Detects if the project uses Docker and returns the appropriate compose file."""
        main_docker_files = [
            os.path.join(proj_path, 'local.yml'),
            os.path.join(proj_path, 'docker-compose.local.yml'),
            os.path.join(proj_path, 'docker-compose.yml'),
            os.path.join(proj_path, 'production.yml'),
            os.path.join(proj_path, 'Dockerfile'),
        ]
        
        uses_docker = any(os.path.exists(docker_file) for docker_file in main_docker_files)
        
        if not uses_docker:
            return False, None
        
        # Determine compose file (prioritize development)
        compose_file = "local.yml"
        if os.path.exists(os.path.join(proj_path, 'docker-compose.local.yml')):
            compose_file = "docker-compose.local.yml"
        elif os.path.exists(os.path.join(proj_path, 'docker-compose.yml')):
            compose_file = "docker-compose.yml"
        elif os.path.exists(os.path.join(proj_path, 'production.yml')):
            compose_file = "production.yml"
        
        return True, compose_file
    
    def _add_security_tools(self, proj_path: str):
        """Adds security and development tools to requirements."""
        logging.info("üîí Adding security and development tools...")
        
        # Search for requirements files
        req_paths = [
            os.path.join(proj_path, 'requirements', 'local.txt'),
            os.path.join(proj_path, 'requirements', 'dev.txt'),
            os.path.join(proj_path, 'requirements-dev.txt'),
            os.path.join(proj_path, 'requirements', 'development.txt'),
        ]
        
        dev_req = next((p for p in req_paths if os.path.exists(p)), None)
        
        # Tools specific to Django
        tools = [
            'bandit[toml]>=1.7.0',  # Security analysis
            'django-debug-toolbar>=4.0.0',  # Debug toolbar
            'django-extensions>=3.2.0',  # Useful extensions
            'safety>=2.3.0',  # Vulnerable dependency checker
            'pre-commit>=3.0.0',  # Git hooks
        ]
        
        if dev_req:
            with open(dev_req, 'r+') as f:
                content = f.read()
                added_tools = []
                for tool in tools:
                    if tool not in content:
                        f.write(f"\n{tool}\n")
                        added_tools.append(tool.split('>=')[0])
                
                if added_tools:
                    logging.info(f"‚úÖ Tools added: {', '.join(added_tools)}")
    
    def _optimize_dockerfile(self, proj_path: str):
        """Optimizes Dockerfile for pip caching."""
        logging.info("üê≥ Optimizing Dockerfile...")
        
        dockerfile_paths = [
            os.path.join(proj_path, 'Dockerfile'),
            os.path.join(proj_path, 'compose', 'local', 'django', 'Dockerfile'),
            os.path.join(proj_path, 'compose', 'production', 'django', 'Dockerfile'),
        ]
        
        for dockerfile in dockerfile_paths:
            if not os.path.exists(dockerfile):
                continue
                
            with open(dockerfile, 'r') as f:
                content = f.read()
            
            # Optimize pip cache
            original_content = content
            content = content.replace('pip install --no-cache -r', 'pip install -r')
            content = content.replace('pip install --no-cache ', 'pip install ')
            
            if content != original_content:
                with open(dockerfile, 'w') as f:
                    f.write(content)
                logging.info(f"‚úÖ Dockerfile optimized: {os.path.relpath(dockerfile)}")
    def _update_docker_compose(self, proj_path: str):
        """Configures cache volumes in docker-compose."""
        logging.info("üêã Configuring cache volumes...")
        
        compose_files = [
            os.path.join(proj_path, 'docker-compose.yml'),
            os.path.join(proj_path, 'local.yml'),
            os.path.join(proj_path, 'production.yml'),
        ]
        
        for compose_file in compose_files:
            if not os.path.exists(compose_file):
                continue
            
            # Try yq first, then fallback to Python YAML
            if hasattr(self, '_has_yq') and self._has_yq:
                self._update_docker_compose_with_yq(compose_file, proj_path)
            else:
                self._update_docker_compose_with_python(compose_file)
    
    def _update_docker_compose_with_yq(self, compose_file: str, proj_path: str):
        """Updates docker-compose.yml using yq command."""
        # Typical services in Django cookiecutter projects
        services_to_update = ['django', 'web', 'app']
        
        for service in services_to_update:
            volumes_expr = f'.services.{service}.volumes += ["$HOME/.cache/pip:/root/.cache/pip","$HOME/.npm:/root/.npm"]'
            try:
                subprocess.check_call([
                    'yq', 'eval', volumes_expr,
                    '-i', os.path.basename(compose_file)
                ], cwd=proj_path, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                logging.info(f"‚úÖ Volumes configured in {os.path.basename(compose_file)} (yq)")
                return  # Success with yq
            except subprocess.CalledProcessError:
                # Continue if the service doesn't exist
                continue
        
        # If yq failed for all services, try Python fallback
        logging.warning("Could not update docker-compose.yml with yq, trying Python fallback")
        self._update_docker_compose_with_python(compose_file)
    
    def _update_docker_compose_with_python(self, compose_file: str):
        """Updates docker-compose.yml using Python YAML library."""
        try:
            import yaml
            
            # Read the docker-compose.yml file
            with open(compose_file, 'r') as f:
                compose_data = yaml.safe_load(f)
            
            # Define cache volumes to add
            cache_volumes = [
                "$HOME/.cache/pip:/root/.cache/pip",
                "$HOME/.npm:/root/.npm"
            ]
            
            # Add volumes to services that exist (Django cookiecutter services)
            services_to_update = ['django', 'web', 'app']
            updated = False
            
            if 'services' in compose_data:
                for service_name in services_to_update:
                    if service_name in compose_data['services']:
                        service = compose_data['services'][service_name]
                        if 'volumes' not in service:
                            service['volumes'] = []
                        
                        # Add cache volumes if they don't exist
                        for volume in cache_volumes:
                            if volume not in service['volumes']:
                                service['volumes'].append(volume)
                                updated = True
            
            # Write back the modified compose file
            if updated:
                with open(compose_file, 'w') as f:
                    yaml.dump(compose_data, f, default_flow_style=False, sort_keys=False)
                logging.info(f"‚úÖ Cache volumes configured in {os.path.basename(compose_file)} (Python)")
            else:
                logging.info(f"‚ÑπÔ∏è  Cache volumes already configured in {os.path.basename(compose_file)}")
                
        except Exception as e:
            logging.warning(f"Could not update {os.path.basename(compose_file)}: {e}")
    
    def _create_precommit_config(self, proj_path: str):
        """Creates pre-commit configuration."""
        logging.info("üé® Creating pre-commit configuration...")
        
        precommit_config = """# .pre-commit-config.yaml generated automatically
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict

  - repo: https://github.com/psf/black
    rev: 23.1.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=88, --extend-ignore=E203]

  - repo: https://github.com/pycqa/bandit
    rev: 1.7.4
    hooks:
      - id: bandit
        args: ['-iii', '-ll']
"""
        
        config_path = os.path.join(proj_path, '.pre-commit-config.yaml')
        with open(config_path, 'w') as f:
            f.write(precommit_config)
        logging.info("‚úÖ Pre-commit configuration created")
    
    def _setup_development_settings(self, proj_path: str):
        """Configures development settings."""
        logging.info("‚öôÔ∏è Configuring development settings...")
        
        settings_paths = [
            os.path.join(proj_path, 'config', 'settings', 'local.py'),
            os.path.join(proj_path, os.path.basename(proj_path), 'settings', 'local.py'),
            os.path.join(proj_path, 'settings', 'local.py'),
        ]
        
        for settings_path in settings_paths:
            if os.path.exists(settings_path):
                with open(settings_path, 'r') as f:
                    content = f.read()
                
                # Add useful development settings
                additions = []
                
                if 'django_extensions' not in content:
                    additions.append("INSTALLED_APPS += ['django_extensions']")
                
                if 'debug_toolbar' not in content and 'DEBUG_TOOLBAR' not in content:
                    additions.append("""
# Django Debug Toolbar
if DEBUG:
    INSTALLED_APPS += ['debug_toolbar']
    MIDDLEWARE += ['debug_toolbar.middleware.DebugToolbarMiddleware']
    INTERNAL_IPS = ['127.0.0.1', '10.0.2.2']
""")
                
                if additions:
                    with open(settings_path, 'a') as f:
                        f.write('\n\n# Automatically added settings\n')
                        f.write('\n'.join(additions))
                    logging.info("‚úÖ Development settings updated")
                break
    def _create_pipeline(self, proj_path: str):
        """Creates multiple pipeline.yml files for different modes (dev/test/prod)."""
        logging.info("‚öôÔ∏è Creating multi-mode pipelines...")
        
        uses_docker, compose_file = self._detect_docker_usage(proj_path)
        project_name = os.path.basename(proj_path)
        
        if not uses_docker:
            logging.warning("‚ö†Ô∏è Project without Docker detected - Pipelines will not be created")
            return
        
        # Create development pipeline
        dev_pipeline = DJANGO_PIPELINE_DEV_TEMPLATE.format(
            project_name=project_name,
            compose_file=compose_file
        )
        dev_pipeline_path = os.path.join(proj_path, 'pipeline-dev.yml')
        with open(dev_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(dev_pipeline)
        
        # Create testing pipeline
        test_pipeline = DJANGO_PIPELINE_TEST_TEMPLATE.format(
            project_name=project_name,
            compose_file=compose_file
        )
        test_pipeline_path = os.path.join(proj_path, 'pipeline-test.yml')
        with open(test_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(test_pipeline)
        
        # Create production pipeline
        prod_pipeline = DJANGO_PIPELINE_PROD_TEMPLATE.format(
            project_name=project_name,
            compose_file=compose_file
        )
        prod_pipeline_path = os.path.join(proj_path, 'pipeline-prod.yml')
        with open(prod_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(prod_pipeline)
        
        # Create default pipeline (dev mode for backward compatibility)
        default_pipeline_path = os.path.join(proj_path, 'pipeline.yml')
        with open(default_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(dev_pipeline)
        
        logging.info("‚úÖ Multi-mode pipelines created:")
        logging.info("   üìù pipeline-dev.yml (Development - keeps services running)")
        logging.info("   üß™ pipeline-test.yml (Testing - full E2E with cleanup)")
        logging.info("   üöÄ pipeline-prod.yml (Production - monitoring & performance)")
        logging.info("   üìÅ pipeline.yml (Default - development mode)")

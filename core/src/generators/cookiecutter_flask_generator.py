"""
Flask project generator based on Cookiecutter.
Replaces the previous generator with a cookiecutter template-based approach.
"""
import os
import sys
import subprocess
import tempfile
import shutil
import json
import logging
import stat
import hashlib
import time
from pathlib import Path
from core.src.generators.base_generator import BaseProjectGenerator
from core.src.utils.generator_utils import (
    DependencyManager, TemplateManager, FileOperations, 
    DockerUtils, SecurityTools, PipelineGenerator, _run_command_safe, _check_command_availability, _get_command_executable
)
from core.src.generators.templates.flask_pipeline_templates import (
    FLASK_PIPELINE_DEV_TEMPLATE,
    FLASK_PIPELINE_TEST_TEMPLATE,
    FLASK_PIPELINE_PROD_TEMPLATE
)


class CookiecutterFlaskGenerator(BaseProjectGenerator):
    """Flask generator using cookiecutter with automatic improvements."""
    
    # Flask-specific reserved words (common ones are in ProjectValidator.COMMON_RESERVED_NAMES)
    FLASK_RESERVED_NAMES = {
        'blueprints', 'extensions', 'migrations', 'instance', 'wsgi.py',
        'gunicorn', 'celery', 'redis', 'sqlalchemy', 'alembic'
    }
    
    def __init__(self, project_name: str, output_dir: str, template_url: str = None, interactive: bool = False):
        """
        Initializes the cookiecutter generator.
        
        Args:
            project_name: Name of the project to generate
            output_dir: Directory where the project will be created
            template_url: URL of the cookiecutter template (optional)
            interactive: Whether to use cookiecutter's interactive mode
        """
        # Use centralized validation with Flask-specific reserved words
        super().__init__(project_name, output_dir, self.FLASK_RESERVED_NAMES)
        self.template_url = template_url or 'https://github.com/cookiecutter-flask/cookiecutter-flask.git'
        self.interactive = interactive
        
        # Setup local template cache
        self.local_templates_dir = os.path.join(output_dir, '.cookiecutter_templates')
        self.template_cache_path = None
        
        self._ensure_dependencies()
    
    @staticmethod
    def _handle_remove_readonly(func, path, exc):
        """
        Error handler for Windows permission issues when removing files.
        """
        if os.path.exists(path):
            # Make the file writable and try again
            os.chmod(path, stat.S_IWRITE)
            func(path)
    
    @staticmethod
    def _safe_rmtree(path):
        """
        Safely remove a directory tree, handling Windows permission issues.
        """
        if not os.path.exists(path):
            return
            
        try:
            shutil.rmtree(path)
        except (OSError, PermissionError):
            # Try again with error handler for Windows
            try:
                shutil.rmtree(path, onerror=CookiecutterFlaskGenerator._handle_remove_readonly)
            except Exception as e:
                logging.warning(f"Could not remove temporary directory {path}: {e}")
    
    def get_project_type(self) -> str:
        """Returns the project type."""
        return "flask-cookiecutter"
    
    def get_directory_structure(self) -> list:
        """Not applicable for cookiecutter - structure is defined by the template."""
        return []
    
    def get_project_files(self) -> dict:
        """Not applicable for cookiecutter - files are generated by the template."""
        return {}
    
    def create_project(self) -> bool:
        """
        Creates the project using cookiecutter and applies automatic improvements.
        
        Returns:
            bool: True if the project was created successfully, False otherwise
        """
        try:
            logging.info(f"🚀 Generating Flask project '{self.project_name}' with cookiecutter...")
            
            # 1. Generate base project with cookiecutter
            project_path = self._generate_with_cookiecutter()
              # 2. Apply automatic improvements
            logging.info("🔧 Applying automatic improvements...")
            self._add_bandit(project_path)
            self._optimize_dockerfile(project_path)
            self._fix_dockerfile_permissions(project_path)
            self._update_docker_compose(project_path)
            self._fix_shell_script_line_endings(project_path)
            self._create_pipeline(project_path)
            
            logging.info(f"✅ Project '{self.project_name}' created successfully at {project_path}")
            return True
            
        except Exception as e:
            logging.error(f"❌ Error creating project: {e}")
            return False
    def _ensure_dependencies(self):
        """Ensures that the necessary dependencies are installed using centralized dependency manager."""
        # Use centralized dependency management
        cookiecutter_available = DependencyManager.ensure_cookiecutter()
        pyyaml_available = DependencyManager.ensure_pyyaml()
        git_available = DependencyManager.ensure_git()
        
        if not cookiecutter_available:
            raise Exception("Cookiecutter is required but could not be installed")
        
        if not git_available:
            raise Exception(
                "Git is required to clone cookiecutter templates but is not available. "
                "Please install Git and ensure it's in your PATH."
            )
        
        if not pyyaml_available:
            logging.warning("PyYAML not available - some features may be limited")
        
        # Check for yq (optional, with fallback)
        self._has_yq = DependencyManager.check_system_command('yq')
        if not self._has_yq:
            logging.info("ℹ️  yq not available on Windows, using Python fallback")
    
    def _apply_custom_config(self, custom_config: dict):
        """
        Applies custom configuration for cookiecutter.
        
        Args:
            custom_config: Dictionary with custom values from the form
        """
        self.custom_config = custom_config
    
    def _prepare_template(self) -> str:
        """
        Prepares the template for use with local caching system.
        
        Returns:
            str: Path to the prepared template (either cached or newly cloned)
        """
        try:
            # If it's a local path, use it directly
            if not self.template_url.startswith(('http://', 'https://', 'git@')) and not self.template_url.endswith('.git'):
                if os.path.exists(self.template_url):
                    logging.info(f"📂 Using local template: {self.template_url}")
                    return self.template_url
                else:
                    raise FileNotFoundError(f"Local template not found: {self.template_url}")
            
            # For remote templates, use local caching
            return self._get_or_clone_template()
            
        except Exception as e:
            raise Exception(f"Error preparing template: {e}")
    
    def _get_or_clone_template(self) -> str:
        """
        Gets the template from local cache or clones it if not present.
        
        Returns:
            str: Path to the cached template
        """
        # Create cache directory if it doesn't exist
        os.makedirs(self.local_templates_dir, exist_ok=True)
        
        # Generate cache key from template URL
        template_hash = hashlib.md5(self.template_url.encode()).hexdigest()[:10]
        template_name = self.template_url.split('/')[-1].replace('.git', '')
        cache_dir_name = f"{template_name}_{template_hash}"
        
        self.template_cache_path = os.path.join(self.local_templates_dir, cache_dir_name)
        
        # Check if template is already cached and valid
        if self._is_template_cache_valid():
            logging.info(f"📋 Using cached template: {cache_dir_name}")
            return self.template_cache_path
        
        # Clone or update the template
        return self._clone_template()
    
    def _is_template_cache_valid(self) -> bool:
        """
        Checks if the cached template is valid and up-to-date.
        
        Returns:
            bool: True if cache is valid, False otherwise
        """
        if not os.path.exists(self.template_cache_path):
            return False
        
        # Check if cookiecutter.json exists (basic validity check)
        cookiecutter_json = os.path.join(self.template_cache_path, 'cookiecutter.json')
        if not os.path.exists(cookiecutter_json):
            logging.warning(f"⚠️ Invalid cached template (missing cookiecutter.json): {self.template_cache_path}")
            return False
        
        # Check cache age (optional: refresh if older than 24 hours)
        try:
            cache_info_file = os.path.join(self.template_cache_path, '.cache_info')
            if os.path.exists(cache_info_file):
                with open(cache_info_file, 'r') as f:
                    cache_data = json.load(f)
                
                # Check if cache is older than 24 hours
                cache_time = cache_data.get('timestamp', 0)
                current_time = time.time()
                cache_age_hours = (current_time - cache_time) / 3600
                
                if cache_age_hours > 24:
                    logging.info(f"🔄 Template cache is {cache_age_hours:.1f}h old, will refresh")
                    return False
        except Exception as e:
            logging.warning(f"⚠️ Error reading cache info: {e}")
        
        return True
    
    def _clone_template(self) -> str:
        """
        Clones the template to the local cache directory.
        
        Returns:
            str: Path to the cloned template
        """
        # Remove existing cache if present
        if os.path.exists(self.template_cache_path):
            logging.info(f"🗑️ Removing old cached template: {self.template_cache_path}")
            FileOperations.safe_rmtree(self.template_cache_path)
        
        # Ensure git is available
        if not _check_command_availability('git'):
            raise Exception(
                "Git is required to clone cookiecutter templates but is not available. "
                "Please install Git and ensure it's in your PATH."
            )
        
        logging.info(f"📥 Cloning template to local cache: {self.template_url}")
        
        try:
            # Clone the template
            result = _run_command_safe([
                'git', 'clone', '--depth', '1', self.template_url, self.template_cache_path
            ], stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, timeout=120)
            
            if result.returncode != 0:
                error_msg = result.stderr.decode() if result.stderr else f"Git clone failed with exit code {result.returncode}"
                raise subprocess.CalledProcessError(result.returncode, 'git clone', stderr=error_msg)
            
            # Create cache info file
            self._create_cache_info()
            
            logging.info(f"✅ Template cached successfully: {os.path.basename(self.template_cache_path)}")
            return self.template_cache_path
            
        except subprocess.TimeoutExpired:
            error_msg = f"Git clone timed out for {self.template_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        except subprocess.CalledProcessError as e:
            error_msg = f"Failed to clone template: {e.stderr.decode() if e.stderr else str(e)}"
            logging.error(error_msg)
            raise Exception(error_msg)
        except Exception as e:
            error_msg = f"Unexpected error cloning template: {e}"
            logging.error(error_msg)
            raise Exception(error_msg)
    
    def _create_cache_info(self):
        """Creates a cache info file with metadata about the cached template."""
        try:
            cache_info = {
                'template_url': self.template_url,
                'timestamp': time.time(),
                'cached_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'cache_version': '1.0'
            }
            
            cache_info_file = os.path.join(self.template_cache_path, '.cache_info')
            with open(cache_info_file, 'w') as f:
                json.dump(cache_info, f, indent=2)
                
        except Exception as e:
            logging.warning(f"⚠️ Could not create cache info file: {e}")
    
    def _cleanup_old_templates(self):
        """Cleans up old template caches to save disk space."""
        try:
            if not os.path.exists(self.local_templates_dir):
                return
            
            current_time = time.time()
            for item in os.listdir(self.local_templates_dir):
                item_path = os.path.join(self.local_templates_dir, item)
                if os.path.isdir(item_path):
                    # Check age of directory
                    dir_age = current_time - os.path.getctime(item_path)
                    # Remove caches older than 7 days
                    if dir_age > (7 * 24 * 3600):
                        logging.info(f"🗑️ Removing old template cache: {item}")
                        FileOperations.safe_rmtree(item_path)
                        
        except Exception as e:
            logging.warning(f"⚠️ Error during template cleanup: {e}")
    
    def _generate_with_cookiecutter(self) -> str:
        """
        Generates the project using cookiecutter with local template caching.
        
        Returns:
            str: Path of the generated project
        """
        # Clean up old templates before starting
        self._cleanup_old_templates()
        
        # Prepare template (cached or clone)
        template_dir = self._prepare_template()
        
        # Check that cookiecutter.json exists
        config_path = os.path.join(template_dir, 'cookiecutter.json')
        if not os.path.exists(config_path):
            raise Exception("cookiecutter.json not found in the template")
        
        # Temporary output directory
        tmp_out = tempfile.mkdtemp(prefix='cc_flask_')
        
        try:
            if self.interactive:
                # Interactive mode
                self._run_interactive_cookiecutter(template_dir, tmp_out)
            else:
                # Automatic mode with default or custom values
                self._run_automated_cookiecutter(template_dir, tmp_out)
            
            # Move generated project to final destination
            generated_path = self._move_generated_project(tmp_out)
            return generated_path
            
        finally:
            # Clean up temporary files (but keep template cache)
            if os.path.exists(tmp_out):
                FileOperations.safe_rmtree(tmp_out)
    
    def _run_interactive_cookiecutter(self, template_dir: str, output_dir: str):
        """Runs cookiecutter in interactive mode."""
        cmd = [
            'cookiecutter', template_dir,
            '--output-dir', output_dir,
            '--overwrite-if-exists'
        ]
        
        print("\n" + "="*50)
        print("🎯 Flask Project Configuration (interactive mode)")
        print("="*50)
        
        # Use safe command execution
        result = _run_command_safe(cmd, timeout=300)
        if result.returncode != 0:
            raise subprocess.CalledProcessError(result.returncode, ' '.join(cmd))
    
    def _run_automated_cookiecutter(self, template_dir: str, output_dir: str):
        """Runs cookiecutter in automated mode with predefined configuration."""
        config_path = os.path.join(template_dir, 'cookiecutter.json')
        
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Prepare replacement values
        project_slug = self.project_name.lower().replace(' ', '_').replace('-', '_')
        defaults = {
            'full_name': 'Developer',
            'email': 'dev@example.com',
            'github_username': 'developer',
            'project_name': self.project_name,
            'app_name': project_slug,
            'project_short_description': f'Flask Application: {self.project_name}',
            'use_pipenv': 'n',
            'python_version': '3.11',
            'node_version': '18',
            'use_heroku': 'n'
        }
        
        # Apply custom configuration if it exists
        if hasattr(self, 'custom_config') and self.custom_config:
            # Auto-fill app_name based on project_name if not defined
            if 'project_name' in self.custom_config and 'app_name' not in self.custom_config:
                self.custom_config['app_name'] = self.custom_config['project_name'].lower().replace(' ', '_').replace('-', '_')
            
            logging.info("🔧 Applying custom configuration from the form")
            defaults.update(self.custom_config)
        
        # Create automatic responses
        input_values = []
        for key in config.keys():
            if key in defaults:
                value = str(defaults[key])
                input_values.append(value)
            else:
                input_values.append('')
        
        input_text = '\n'.join(input_values) + '\n'
        
        # Run cookiecutter with automatic responses
        cmd = ['cookiecutter', template_dir, '--output-dir', output_dir, '--overwrite-if-exists']
        
        try:
            # Use safe command execution with input
            process = subprocess.Popen(
                [_get_command_executable(cmd[0])] + cmd[1:], 
                stdin=subprocess.PIPE, 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE, 
                text=True
            )
            stdout, stderr = process.communicate(input=input_text, timeout=300)
            
            if process.returncode != 0:
                raise Exception(f"Error running cookiecutter: {stderr}")
                
        except subprocess.TimeoutExpired:
            process.kill()
            raise Exception("Cookiecutter execution timed out")
    
    def _move_generated_project(self, temp_output: str) -> str:
        """Moves the generated project to the final destination."""
        entries = [d for d in os.listdir(temp_output) if not d.startswith('.')]
        if not entries:
            raise Exception("No project was generated")
        
        # Find the directory of the generated project
        src = os.path.join(temp_output, entries[0]) if len(entries) == 1 else temp_output
        dest = self.project_path
        
        if dest.exists():
            FileOperations.safe_rmtree(str(dest))
        
        shutil.move(src, str(dest))
        return str(dest)
    
    def _add_bandit(self, project_path: str):
        """Adds Bandit to the development requirements."""
        logging.info("🔒 Adding Bandit for security analysis...")
        
        # Find development requirements file
        dev_req_paths = [
            os.path.join(project_path, 'requirements', 'dev.txt'),
            os.path.join(project_path, 'requirements-dev.txt'),
        ]
        
        dev_req = next((p for p in dev_req_paths if os.path.exists(p)), None)
        if not dev_req:
            logging.warning("Development requirements file not found to add Bandit")
            return
        
        bandit_line = 'bandit[toml]>=1.7.0'
        with open(dev_req, 'r+') as f:
            content = f.read()
            if bandit_line not in content:
                f.write(f"\n{bandit_line}\n")
                logging.info("✅ Bandit added to development requirements")
            else:
                logging.info("ℹ️  Bandit already present in requirements")
    
    def _optimize_dockerfile(self, project_path: str):
        """Optimizes the Dockerfile to use pip cache."""
        logging.info("🐳 Optimizing Dockerfile for pip cache...")
        
        dockerfile_path = os.path.join(project_path, 'Dockerfile')
        if not os.path.exists(dockerfile_path):
            logging.warning("Dockerfile not found")
            return
        
        # Read and modify Dockerfile
        with open(dockerfile_path, 'r') as f:
            lines = f.readlines()
        
        changes_made = False
        for i, line in enumerate(lines):
            original_line = line
            line = line.replace('pip install --no-cache -r', 'pip install -r')
            line = line.replace('pip install --no-cache ', 'pip install ')
            if line != original_line:
                lines[i] = line
                changes_made = True
        
        if changes_made:
            with open(dockerfile_path, 'w') as f:
                f.writelines(lines)
            logging.info("✅ Dockerfile optimized for pip cache")
        else:
            logging.info("ℹ️  Dockerfile already optimized")
    def _update_docker_compose(self, project_path: str):
        """Updates docker-compose.yml to add cache volumes."""
        logging.info("🐋 Configuring cache volumes in docker-compose...")
        
        compose_path = os.path.join(project_path, 'docker-compose.yml')
        if not os.path.exists(compose_path):
            logging.warning("docker-compose.yml not found")
            return
        
        # Try yq first, then fallback to Python YAML
        if hasattr(self, '_has_yq') and self._has_yq:
            self._update_docker_compose_with_yq(compose_path)
        else:
            self._update_docker_compose_with_python(compose_path)
    
    def _update_docker_compose_with_yq(self, compose_path: str):
        """Updates docker-compose.yml using yq command."""
        import sys
        
        if sys.platform == 'win32':
            # Windows: Skip cache volumes to avoid HOME variable issues
            logging.info("ℹ️  Skipping cache volumes on Windows to avoid HOME variable issues")
            return
        
        # Unix/Linux/macOS
        volumes_expr = (
            '.services.flask-dev.volumes += ["$HOME/.cache/pip:/root/.cache/pip","$HOME/.npm:/root/.npm"] | '
            '.services.flask-prod.volumes += ["$HOME/.cache/pip:/root/.cache/pip","$HOME/.npm:/root/.npm"] | '
            '.services.manage.volumes += ["$HOME/.cache/pip:/root/.cache/pip","$HOME/.npm:/root/.npm"]'
        )
        
        try:
            subprocess.check_call([
                'yq', 'eval', volumes_expr,
                '-i', compose_path
            ])
            logging.info("✅ Cache volumes configured in docker-compose.yml (yq)")
        except subprocess.CalledProcessError:
            logging.warning("Could not update docker-compose.yml with yq, trying Python fallback")
            self._update_docker_compose_with_python(compose_path)

    def _update_docker_compose_with_python(self, compose_path: str):
        """Updates docker-compose.yml using Python YAML library."""
        try:
            import yaml
            
            # Read the docker-compose.yml file
            with open(compose_path, 'r') as f:
                compose_data = yaml.safe_load(f)            # Define cache volumes to add (cross-platform compatible)
            import os
            import sys
            
            if sys.platform == 'win32':
                # Windows: Use simpler approach to avoid HOME variable issues
                # We'll skip cache volumes on Windows to prevent the warning
                cache_volumes = []
                logging.info("ℹ️  Skipping cache volumes on Windows to avoid HOME variable issues")
            else:
                # Unix/Linux/macOS
                cache_volumes = [
                    "$HOME/.cache/pip:/root/.cache/pip",
                    "$HOME/.npm:/root/.npm"
                ]
            
            # Add volumes to services that exist
            services_to_update = ['flask-dev', 'flask-prod', 'manage']
            updated = False
            
            if 'services' in compose_data:
                for service_name in services_to_update:
                    if service_name in compose_data['services']:
                        service = compose_data['services'][service_name]
                        if 'volumes' not in service:
                            service['volumes'] = []
                        
                        # Add cache volumes if they don't exist
                        for volume in cache_volumes:
                            if volume not in service['volumes']:
                                service['volumes'].append(volume)
                                updated = True
            
            # Write back the modified compose file
            if updated:
                with open(compose_path, 'w') as f:
                    yaml.dump(compose_data, f, default_flow_style=False, sort_keys=False)
                logging.info("✅ Cache volumes configured in docker-compose.yml (Python)")
            else:
                logging.info("ℹ️  Cache volumes already configured in docker-compose.yml")
                
        except Exception as e:
            logging.warning(f"Could not update docker-compose.yml: {e}")

    def _fix_shell_script_line_endings(self, proj_path: str):
        """Fix Windows line endings in shell scripts to prevent Docker execution errors."""
        logging.info("🔧 Fixing shell script line endings...")
        
        # Find all shell script files
        shell_script_patterns = [
            os.path.join(proj_path, 'shell_scripts', '*.sh'),
            os.path.join(proj_path, '*.sh'),
            os.path.join(proj_path, 'scripts', '*.sh'),
        ]
        
        import glob
        import stat
        fixed_count = 0
        
        for pattern in shell_script_patterns:
            for script_path in glob.glob(pattern):
                try:
                    # Read the file in binary mode to preserve exact content
                    with open(script_path, 'rb') as f:
                        content = f.read()
                    
                    # Check if file has Windows line endings
                    if b'\r\n' in content:
                        # Convert Windows line endings to Unix
                        fixed_content = content.replace(b'\r\n', b'\n')
                        
                        # Write back with Unix line endings
                        with open(script_path, 'wb') as f:
                            f.write(fixed_content)
                        
                        fixed_count += 1
                        logging.info(f"✅ Fixed line endings in {os.path.basename(script_path)}")
                    
                    # Make shell script executable (important for Docker)
                    current_mode = os.stat(script_path).st_mode
                    os.chmod(script_path, current_mode | stat.S_IEXEC | stat.S_IXUSR | stat.S_IXGRP)
                
                except Exception as e:
                    logging.warning(f"Could not fix line endings in {script_path}: {e}")        # Fix Dockerfile ENTRYPOINT if needed
        dockerfile_path = os.path.join(proj_path, 'Dockerfile')
        if os.path.exists(dockerfile_path):
            try:
                with open(dockerfile_path, 'r', encoding='utf-8') as f:
                    dockerfile_content = f.read()
                
                # Fix bash/sh inconsistency in ENTRYPOINT
                if 'ENTRYPOINT ["/bin/bash", "shell_scripts/supervisord_entrypoint.sh"]' in dockerfile_content:
                    dockerfile_content = dockerfile_content.replace(
                        'ENTRYPOINT ["/bin/bash", "shell_scripts/supervisord_entrypoint.sh"]',
                        'ENTRYPOINT ["shell_scripts/supervisord_entrypoint.sh"]'
                    )
                    
                    with open(dockerfile_path, 'w', encoding='utf-8') as f:
                        f.write(dockerfile_content)
                    
                    logging.info("✅ Fixed Dockerfile ENTRYPOINT shell consistency")
                
            except Exception as e:
                logging.warning(f"Could not fix Dockerfile ENTRYPOINT: {e}")
        
        if fixed_count > 0:
            logging.info(f"🔧 Fixed line endings in {fixed_count} shell script(s)")
        else:
            logging.info("ℹ️  No shell scripts found or all already have correct line endings")

        
    def _create_pipeline(self, proj_path: str):
        """Creates multiple pipeline.yml files for different modes (dev/test/prod)."""
        logging.info("⚙️ Creating multi-mode pipelines...")
        
        project_name = os.path.basename(proj_path)
        
        # Create development pipeline
        dev_pipeline = FLASK_PIPELINE_DEV_TEMPLATE.format(
            project_name=project_name,
        )
        dev_pipeline_path = os.path.join(proj_path, 'pipeline-dev.yml')
        with open(dev_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(dev_pipeline)
        
        # Create testing pipeline
        test_pipeline = FLASK_PIPELINE_TEST_TEMPLATE.format(
            project_name=project_name,
        )
        test_pipeline_path = os.path.join(proj_path, 'pipeline-test.yml')
        with open(test_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(test_pipeline)
        
        # Create production pipeline
        prod_pipeline = FLASK_PIPELINE_PROD_TEMPLATE.format(
            project_name=project_name,
        )
        prod_pipeline_path = os.path.join(proj_path, 'pipeline-prod.yml')
        with open(prod_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(prod_pipeline)
        
        # Create default pipeline (dev mode for backward compatibility)
        default_pipeline_path = os.path.join(proj_path, 'pipeline.yml')
        with open(default_pipeline_path, 'w', encoding='utf-8') as f:
            f.write(dev_pipeline)
        
        logging.info("✅ Multi-mode pipelines created:")
        logging.info("   📝 pipeline-dev.yml (Development - keeps services running)")
        logging.info("   🧪 pipeline-test.yml (Testing - full E2E with cleanup)")
        logging.info("   🚀 pipeline-prod.yml (Production - monitoring & performance)")
        logging.info("   📁 pipeline.yml (Default - development mode)")
    
    def _fix_dockerfile_permissions(self, project_path: str):
        """Fixes Docker permission issues by reordering commands in production stage."""
        logging.info("🔧 Fixing Dockerfile permission order...")
        
        dockerfile_path = os.path.join(project_path, 'Dockerfile')
        if not os.path.exists(dockerfile_path):
            logging.warning("Dockerfile not found")
            return
        
        with open(dockerfile_path, 'r') as f:
            content = f.read()
        
        # Split into lines for analysis
        lines = content.split('\n')
        production_start = -1
        user_line_idx = -1
        chmod_line_idx = -1
        copy_dot_idx = -1
        modified = False
        
        # Find production stage and relevant lines
        for i, line in enumerate(lines):
            # Fix casing issue in FROM statements only
            if 'FROM' in line and ' as ' in line:
                lines[i] = line.replace(' as ', ' AS ')
                modified = True
            
            if 'AS production' in line:
                production_start = i
            elif production_start >= 0:  # We're in production stage
                if line.strip().startswith('USER '):
                    user_line_idx = i
                elif 'chmod +x shell_scripts' in line:
                    chmod_line_idx = i
                elif line.strip() == 'COPY . .':
                    copy_dot_idx = i
        
        # Check if we need to add or reorder chmod command
        if production_start >= 0 and copy_dot_idx >= 0 and user_line_idx >= 0:
            if chmod_line_idx == -1:
                # chmod command is missing - add it after COPY . . but before USER
                logging.info("📝 Adding missing chmod command for shell scripts...")
                lines.insert(copy_dot_idx + 1, "")
                lines.insert(copy_dot_idx + 2, "# Make shell scripts executable before switching to non-root user")
                lines.insert(copy_dot_idx + 3, "RUN chmod +x shell_scripts/*.sh")
                modified = True
                
            elif chmod_line_idx > user_line_idx:
                # chmod command exists but comes after USER - reorder it
                logging.info("🔄 Reordering chmod command to run before USER...")
                
                # Remove the chmod line from its current position
                chmod_command = lines[chmod_line_idx].strip()
                lines.pop(chmod_line_idx)
                
                # Insert chmod after COPY . . but before USER
                lines.insert(copy_dot_idx + 1, "")
                lines.insert(copy_dot_idx + 2, "# Make shell scripts executable before switching to non-root user")
                lines.insert(copy_dot_idx + 3, chmod_command)
                modified = True
        
        # Write back the fixed content if modifications were made
        if modified:
            with open(dockerfile_path, 'w') as f:
                f.write('\n'.join(lines))
            logging.info("✅ Dockerfile permission issues fixed")
        else:
            logging.info("ℹ️  Dockerfile permission order already correct")
